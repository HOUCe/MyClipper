---
created: 2021-10-11
tags: []
source: https://kaiwu.lagou.com/course/courseInfo.htm?courseId=820#/detail/pc?id=7134
author: 
---

# [玩转 Python 数据分析 - BAT 高级技术专家，前创业公司 CTO - 拉勾教育](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=820#/detail/pc?id=7134)


我们在前面的部分里，已经学习完了 NumPy 的基础操作和用法。我们在前文中提到过，NumPy 是数值分析的神器，现在神器已经学得差不多了。现在我们来学习数值分析中最常见的一类分析方式：回归分析。

### 什么是回归分析

回归分析，本质是在寻找不同变量之间的关系。

这里的变量和 Python 的变量不太一样，这里的变量指的是统计学中的变量，统计学的变量指的是说明事物某种特征的量，变量往往会根据多次的观测，有多个值。

举个例子，我们希望统计房屋的地段、面积、楼层和房价的关系。这里面的地段、面积、楼层和房价都是变量。回归分析是怎么确定他们之间的关系的呢？答案是：通过大量的观测。

观测（也可以说是观察）也是一个统计学的说法，整体来说指的就是通过搜集现有的数据。比如我们搜集不同地区，不同楼层的房子的数据，整理成一个表格。每个变量都是一列：地段、面积、楼层、房价。我们希望找地段、面积、楼层和单价的关系，简单来说就是找到一个这样的函数：

![Drawing 1.png](https://s0.lgstatic.com/i/image6/M01/44/1F/Cgp9HWC90yGAFgWwAAASh_p7WuY552.png)

f 是什么样形式的函数，原先我们是不知道的，我们有的只是大量的房屋数据。**所谓回归分析，就是通过大量已有的数据，来拟合出对应关系 f。同样对于新的数据，只要我们知道了房屋的地段、面积和楼层，就能够估算出其房价。**

在上面的变量中，地段、面积和楼层称之为自变量，单价称为因变量。

回归分析在现实中有非常多的应用。我们只需要把现实问题用自变量、因变量的思路去建模，就能够使用回归分析找到自变量和因变量的关系，以及用自变量与预测未知的因变量。

比如我们有员工的基本信息，我们就能通过回归分析来分析哪些变量（年龄、性别、籍贯、学历）和员工收入的函数关系。这样之后我们只要知道了员工的这些基本信息，就能够预测出员工的收入。

### 线性回归

#### 定义

回归分析有非常多种，其中最基础、使用最广的就是线性回归。线性回归也是相对来说最容易理解的回归分析方法。

线性回归，顾名思义，就是说自变量和因变量的函数关系是线性关系。以我们上面说的房屋价格问题为例。如果用线性回归来建模这个问题，那这个问题就可以转换为以下公式：

![Drawing 3.png](https://s0.lgstatic.com/i/image6/M01/44/27/CioPOWC90zKAZ7-6AAAiODZlGZA693.png)

为了简化表示，我们用假设 x0 = 地段，x1 = 面积，x2 = 楼层，y = 单价。上述式子可以化简为这样：

![Drawing 5.png](https://s0.lgstatic.com/i/image6/M01/44/27/CioPOWC90zqAO9MdAAAZlaldQYQ341.png)

对于线性回归问题，本质就是要通过已知的一批 (y, x0, x1, x2) 的观测记录，来求得线性函数的四个系数 a、b、c、d 的值。

#### 误差衡量

通过已有的观测来估算 a、b、c、d 往往都不是精准的，会存在一定的误差。那我们怎么衡量我们找到的系数是对的呢？我们首先需要能够描述一组特定的系数，比如：a',b',c',d' 到底有多贴近真实的 abcd。我们把参数等于 a',b',c',d' 的函数 f 记做 f'。

然后假设我们有 m 条观测记录，每条记录包含 y,x0,x1,x2 四个值。a',b',c',d' 这组参数的误差就等于：

![Drawing 10.png](https://s0.lgstatic.com/i/image6/M00/44/28/CioPOWC909mAWicfAAARyC_qDSE409.png)

这个公式也被称为 SSR（Sum of Square Residuals），残差平方和。指的是我们针对每一条记录，用 a',b',c',d' 系数确定的函数 f', 结合 x0，x1，x2 计算出一个 y'。y' 是我们估算系数计算出的估计值，而我们记录中的 y 则是实际值。用估算值减去实际值就得到了这条记录的误差。然后，我们将每条记录的误差求平方并加总起来，就得到了参数 a',b',c',d' 的总误差。

所以，线性回归的问题进一步转化为，**找到一组特定的 a,b,c,d，使得总误差函数的结果最小。** 一般情况下比较常见的方法有梯度下降，由于 Python 已经提供了现成的拟合能力，并不需要我们手工计算，所以如何计算的过程在此就不展开。

#### 线性回归的几何意义

在这一节，我们通过一个直观的例子来说明线性回归希望解决的问题。假设我们只有一个自变量和一个因变量。那么我们的函数形式就是一个类似直线方程的形式：

![Drawing 14.png](https://s0.lgstatic.com/i/image6/M00/44/20/Cgp9HWC91AmAMR7GAAAP9kQi9TM833.png)

假设我们有收集到 5 组（x,y）的观测记录，分别如下：

x

y

1.0

35.1

2.0

42.9

3.0

39.7

4.0

41.5

5.0

49.3

那我们希望用线性回归解决的问题就是通过这五组记录，估算出方程中的 k 和 b。

首先我们把每组 (x,y) 都当成平面上的一个点。那这五个点画出来就是像下面这样的一张图：

![Drawing 15.png](https://s0.lgstatic.com/i/image6/M00/44/28/CioPOWC91BSAdoU1AABtdkSC9wg911.png)

我们要寻找的 k 和 b，本质上就是希望寻找到一条直线，尽可能地靠近这些点。比如可以这样画：

![Drawing 16.png](https://s0.lgstatic.com/i/image6/M00/44/28/CioPOWC91BqAZhbSAACqo_DPJJ4908.png)

也可以这样画：

![Drawing 17.png](https://s0.lgstatic.com/i/image6/M00/44/28/CioPOWC91CGAe7YUAACz4WHFNjI434.png)

那我们到底怎么衡量哪条直线是最好的呢？一般来说，常见的方法是统计每个点到直线的距离，再将这些距离加起来。哪条线的距离和最小，说明哪条线就是最优的，具体就是下图中的五条小竖线：

![Drawing 18.png](https://s0.lgstatic.com/i/image6/M00/44/20/Cgp9HWC91C6Afl9gAAC09qVeM_8509.png)

线上的点和我们观测的点，x 是一样的。相减等于 0，所以我们只需要比较两个 y 的差即可。也就是说，对于特定的 k', b'，小竖线的长度总和可以表示为如下公式：

![Drawing 20.png](https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91EyAaTLWAAAcG-ceX5U274.png)

我们让 i 从 1 到 5 循环，来分别计算每个 x 对应的 y 和 y' 的距离并加总。但这样会有一个问题：有的 y 比 y' 大，比如第二个点，有的 y 却比 y' 小，比如第三个点。但我们感兴趣的只是距离，这样就会因为符号问题导致计算出现误差。解决的办法也很简单，我们直接把两个 y 相减加上一个平方，这样就不会有符号问题了。

所以公式修改如下：

![Drawing 22.png](https://s0.lgstatic.com/i/image6/M00/44/20/Cgp9HWC91GeAEk3nAAARFiC0Hp0792.png)

上面的公式看着是不是很眼熟？没错，这就是前文所讲的残差平方和的公式。讲到这里，相信你对于线性回归的误差衡量有了一个更加直观的感受。

### 用 Python 实现线性回归

上面我介绍了什么是线性回归，线性回归能解决什么样的问题，以及如何衡量回归分析的结果好不好。相信你一定已经迫不及待地想要学习到底怎么做回归分析，怎么从观测记录中计算线性函数的系数。

本章我们以一个具体的例子来介绍如何使用 Python 完成回归分析的计算。

回归分析的数学原理，简单地来说就是通过对误差函数求导的形式来做梯度下降，不断迭代出最佳的系数（比如一元线性回归的 k 和 b）。这里涉及比较多数学原理，我就不再展开。我们直接使用 Python 提供的方案即可完成。

#### 环境准备

为了实现回归分析，我们需要安装一个新的工具包：scikit-learn 来和 NumPy 配合完成。

我们按照之前的方式，打开开始菜单→ Anaconda3 → Anaconda3 Prompt ，输入 conda install scikit-learn 之后回车，出现以下界面：

![Drawing 23.png](https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91HWAIs99AAEC1AP9Gog021.png)

输入 y 回车，完成安装。

#### 任务描述

众所周知，房子的价格很大程度取决于地段。我们观测了十套房子的单价以及它们距离市中心的距离。如下所示：

距离市中心的距离（公里）

单价（万元）

1.0

10.14

2.0

9.89

3.0

8.41

4.0

9.61

5.0

7.95

6.0

7.46

7.0

6.62

8.0

5.23

9.0

4.70

10.0

4.77

假设距离市中心的距离和单价存在线性关系。尝试基于上述数据进行回归分析，求得线性函数的系数，并预测距离市区 15 公里的房屋单价。

#### 问题分析

我们希望通过距离来预测单价，所以这是一个典型的一元线性回归问题。距离是自变量，记做 x。单价是因变量，记做 y。所以我们要求的函数就是：

![Drawing 25.png](https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91IGAJdKkAAAMBrlsUXI865.png)

k 和 b 就是线性函数的系数，只要求得 k 和 b，我们只要把 x 带入 15.0，就能得到距离市区 15 公里的地方的单价的预测值。

#### 任务实现

（1）载入数据。我们把已经列好的数组分别用两个多维数组来存储。每个数组都是 1 维即可。

```
x = np.array([1.0, 2.0, 3.0, 4.0, 5.0 ,6.0,7.0,8.0,9.0,10])
y = np.array([10.14 ,  9.89 ,  8.41,  9.61,  7.95,
7.46,  6.62,  5.23,  4.70,  4.77])
```

（2）实现画图函数。为了方便分析，我们先实现两个工具函数，一个用来画点，一个用来画点+线。由于我们还没有学习 matplotlib，所以代码的含义可以先不用关心，理解函数怎么使用即可。

```
import matplotlib 
import matplotlib.pyplot as plt
def draw(x,y):
    matplotlib.style.use("ggplot")
    plt.scatter(x, y,c='b' )
    plt.show()
def draw_point_line(x,y,y1):
    matplotlib.style.use("ggplot")
    plt.scatter(x, y,c='b' )
    plt.plot(x, y1, c='r')
    plt.show()
```

（3）查看观测数据的分布。我们调用 draw 函数来将我们的观测值画出来，看看是否有具备线性关系。

输出如下：  
![Drawing 26.png](https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91IqAC9PjAABahHLjoZw454.png)

可以看到，确实如任务描述章节所述，随着距离市区的公里数越多（x轴）房屋的单价就越低（y 轴）。而且看起来可以找到一条直线尽可能地靠近目前所有的点。

（4）初始化线性回归器对象。线性回归器（LinearRegression）是 scikit-learn 工具包中的一个类，通过它我们可以非常方便地进行回归分析。

```
from sklearn.linear_model import LinearRegression
model = LinearRegression()
```

（5）调整 x 数组的结构，满足 LinearRegression 类的 fit 函数的要求。

输出如下：

```
array([[ 1.],
       [ 2.],
       [ 3.],
       [ 4.],
       [ 5.],
       [ 6.],
       [ 7.],
       [ 8.],
       [ 9.],
       [10.]])
```

可以看到，x 从一个 10 个元素的一维数组已经被成功转化为 10x1 的二维数组。

（6）实现回归分析，并输出找到的最佳的 k 和 b。

```
model.fit(x ,y)
print("最佳的k:",model.coef_[0])
print("最佳的b:", model.intercept_)
```

输出如下：

```
最佳的k: -0.6667878787878787
最佳的b: 11.145333333333333
```

（7）查看拟合效果。调用 fit 函数进行回归分析，虽然我们总能找到一个针对当前观测记录最优的 k 和 b，但依然可能不是完美的。毕竟观测记录一般都会有一定的误差，从我们之前画的散点图来看，观测记录只能说大概线性，而不是严格线性的（毕竟无法用一条直线把所有点连接起来）。所以每次 fit 完之后，我们都可以调用 score 函数，来看一下分析的效果如何。

```
fit_score = model.score(x, y)
print("fit 的效果：",fit_score)
```

输出如下：

```
fit 的效果： 0.9314051422328029
```

score 返回的是 0 到 1 之间的值，所以 0.93 已经算非常不错的效果，这说明观测记录集本身也是基本线性的。

（8）查看拟合出的直线。在 fit 之后，model 对象内部就记录了最佳的 k 和 b 的信息，之后我们可以调用 predict 函数，传入自变量，就可以获得对应预测出的因变量预测值。所以我们要画出拟合的直线，只需要将我们原始的 x 传入 model，拿到预测的 y ，之后和 x 结对画线即可。

```
y_pred = model.predict(x)
print("观测记录的y：", y)
print("线性模型预测的y：", y_pred)
draw_point_line(x, y, y_pred)
```

输出如下：  
![Drawing 27.png](https://s0.lgstatic.com/i/image6/M00/44/29/CioPOWC91KGAZ9sWAACNJxjxZHY689.png)

可以看到，我们的预测值和原先的观测值仍然是有一定的误差，但也非常接近了。从图来看，我们拟合出的直线也尽可能地贴合了所有的点。

（9）预测距离市中心 15 公里的房价。看到这里，相信这个任务已经非常简单了，我们只需要把 15 以一个 1x1 的二维数字传入 model 对象的 predict 即可。

```
result = model.predict([[15]])
print("距离市中心十五公里的房价是：", result[0])
```

输出如下：

```
距离市中心十五公里的房价是： 1.1435151515151531
```

### 小结

至此，我们基本完成了线性回归分析的学习。

回顾一下今天学习的内容，主要包含如下关键点。

-   回归分析的本质：分析变量之间的关系，通过大量的观测记录，来拟合出自变量到因变量的函数关系 f。
    
-   线性回归：最简单也是最常用的回归，代表自变量和因变量之间是线性关系，以三元回归为例，形式可以表示为 y = ax1 + bx2 +cx3 + d，要求函数关系本质就是求四个参数 a、b、c、d 的值。
    
-   用 Python 实现线性回归：
    
    -   需要安装 scikit-learn 工具包，与 NumPy 配合完成；
        
    -   自变量和因变量的观测记录需要分别存储；
        
    -   创建 LinearRegression 类来进行回归分析的计算。通过 fit 函数可以完成线性函数系数的计算。在使用 fit 函数时，传入的自变量需要是 NxM 的二维数组；
        
    -   fit 之后，可以调用 score 获得拟合效果的打分；
        
    -   fit 之后，调用 predict 可以预测对输入的自变量的值预测出因变量的预测值。
        

课后习题

计算我们第三节拟合的线性模型的残差平方和，以及 k=-1,b = 20 的残差平方和。

___

答案：

```
error1 = np.sum(np.square(y_pred - y))
y_pred2 = -1 * x + 20
error2 = np.sum(np.square(y_pred2 - y))
print("模型拟合的残差平方和：", error1)
print("k=-1,b=20 的残差平方和：",error2)
```

输出如下：

```
模型拟合的残差平方和： 2.7013587878787853
k=-1,b=20 的残差平方和： 6149.662
```

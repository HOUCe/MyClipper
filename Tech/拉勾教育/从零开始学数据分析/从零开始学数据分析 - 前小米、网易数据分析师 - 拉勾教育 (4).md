---
created: 2021-10-11
tags: []
source: https://kaiwu.lagou.com/course/courseInfo.htm?courseId=500#/detail/pc?id=4789
author: 
---

# [从零开始学数据分析 - 前小米、网易数据分析师 - 拉勾教育](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=500#/detail/pc?id=4789)


上一课时我们介绍了 Hive 以及常用技能，今天我们来学习下 Spark。因为 Spark 拥有强大的数据处理能力，近几年越来越多的互联网公司开始使用 Spark，掌握 Spark 基础技能，可以大大扩大你匹配数据分析和处理相关工作岗位的机会。

### 初识 Spark

首先，我们来简单看下 Spark 的背景，它于 2009 年诞生于伯克利大学的 AMPLab，是一款基于内存计算的大数据并行计算框架，被设定为可用于构建一些大型的、低延迟的数据分析的应用程序。

2013 年，Spark 在加入 Apache 孵化器项目后，凭借东风，开始迅速地发展。如今，Spark 已成为 Apache 最重要的三大分布式计算系统开源项目之一（三大分布计算系统为 Hadoop、Spark、Storm）。另一方面，自 2015 年开始， 很多公司部署以 Spark 代替 Hive、MapReduce 的战略。其中 Spark 开发语言 Scala，也因为 Spark 的走火，成为受大厂欢迎的技能。

### Spark 的优势所在

通过上节课我们了解到，Hive 任务和许多任务一样，也是被翻译成 MapReduce 后进行大数据处理的。Spark 相比 MapReduce，**最大的优势就是：快！** 天下武功，唯快不破。Spark 占了“快”这个优点，因此获得了互联网大厂的青睐。

Spark 的快表现在以下两点：

**第一，执行速度快。** 主要原因是 MR 作业对中间处理的数据需要落盘，下次再读取时。磁盘 I/O 成本高。相对而言，基于内存计算的 Spark 就不存在这个问题。对 Spark 而言，中间数据优先放到集群内存，只有数据量大到内存无法保存时，才会保存到磁盘。另外 MR 作业有多个 job， 每个 job 运行完后，下个 job 都需要重新申请计算资源。而 Spark 作业则会一直持续到数据处理结束，中间不需要再向集群申请计算资源。

下面是 Spark 和 MR 性能比较，我们可以看出，对比 MR，Spark 仅使用了十分之一的计算资源，便获得了比 Hadoop 快 3 倍的速度。

![image (1).png](https://s0.lgstatic.com/i/image/M00/59/E7/CgqCHl9y00uACBmVAAMW2KU9DWQ697.png)

**第二，MR 程序编写的复杂度高。** 比如一个基础统计需要几十行程序，Spark 则只需要几行程序，不用像 MR 那样配置各个隐藏在各个函数的操作，简洁高效。它不仅节省人力，而且专注数据逻辑处理。

举个简单的例子。

-   [Java 编写 MR](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
    

```
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
public class WordCount {
public static class TokenizerMapper
extends Mapper<Object, Text, Text, IntWritable>{
private final static IntWritable one = new IntWritable(1);
private Text word = new Text();
public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }
public static class IntSumReducer
extends Reducer<Text,IntWritable,Text,IntWritable> {
private IntWritable result = new IntWritable();
public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
int sum = 0;
for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }
public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
```

-   Spark 进行 WordCount 统计
    

```
val lines = sc.textFile("hdfs路径或本地文件路径")
val res = lines.flatMap(line=> line.split(" ")) \
             .map(word=> (word, 1)) \
             .reduceByKey(_ + _)
res.saveAsTextFile("hdfs://...或本地文件路径")
```

通过代码对比，我们明显能看出，Spark 比 Java 简洁不止一两行代码。

MR 只有 map 和 reduce 两种操作，稍复杂的统计逻辑，就需要通过两种操作组合成多个 job 才能完成，另外还需要复杂的参数配置，最后使程序变得十分臃肿，开发成本很高。 Spark 提供多种转化操作方法，除了 map 和 reduce，还包括 flatMap、filter、union、join、reduceByKey、 sample、zip 等，这使得数据处理代码变得极其简洁。

在互联网公司，时间就是成本，速度在某种程度上限制业务增长的瓶颈，这是老板最不想看到的。Spark 在大数据处理上的优势，不仅能够让数据流动得更快，而且因其语言上的简洁，更加提升了工程师开发效率，让数据挖掘的成本变得更低。大厂积极部署 Spark，反哺 Spark 开源社区，也使得 Spark 开源社区极其活跃，Spark 数据生态也越来越完善。

### Spark 数据生态完善

除了数据处理性能提升，Spark 还提供完善的大数据处理库套装，包括：Spark SQL、Spark Streaming（流式计算）、MLlib（机器学习）、GraphX （图计算）等， 这些功能也让 Spark 相比 Hadoop 优势更大了。

-   **Spark Core**：Spark Core 包含 Spark 的基本功能，如内存计算、任务调度、部署模式、故障恢复、存储管理等。
    
-   **Spark SQL**：Spark SQL 允许开发人员直接处理 RDD，同时也可查询 Hive、 HBase 等外部数据源。 Spark SQL 的一个重要特点是其能够统一处理关系表和 RDD，使得开发人员可以轻松地使用 SQL 命令进行查询，并进行更复杂的数据分析。
    
-   **Spark Streaming**：Spark Streaming 支持高吞吐量、可容错处理的实时流数据处理，其核心思路是将流式计算分解成一系列短小的批处理作业。
    
-   **MLlib（机器学习）**：MLlib 提供了常用机器学习算法的实现，包括聚类、分类、回归、协同过滤等。从而降低了机器学习的门槛，开发人员只要具备一定的理论知识就能进行机器学习的工作。
    
-   **GraphX（图计算）**：GraphX 是 Spark 中用于图计算的 API ，可认为是 Pregel 在 Spark 上的重写及优化，Graphx 性能良好，拥有丰富的功能和运算符，能在海量数据上自如地运行复杂的图算法。
    

以上完善了 Spark 数据生态，让开发者通过 Spark 能挖掘更多数据价值，这也使 Spark 在同类产品中脱颖而出。

### Spark 应用场景

Spark 是一款基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。

Spark 两类应用场景：**离线统计**和**实时计算**。

-   **离线统计**
    

Spark 会应用在一些更底层的更大数据量离线任务中，在相同计算资源条件下，Spark 数据处理性能可以保证报表数据准时生产。

举个例子，亿级日活用户的 App，每个用户使用 App 的产生的数据，包括浏览的内容、转评赞行为等都会形成日志上传。这部分用户的行为数据，需要经过许多层的数据任务处理才能最后呈现到数据分析师、产品、运营等业务方眼前。这时候，保证昨天的数据能被今天业务方及时看到，并推动各自业务制定决策。就显得尤为重要了。传统 Hive 遇到数据量较大的情况，处理效率会变低，这样就使得重要的数据指标延迟很久。昨天的数据有可能要今天下午、晚上甚至第 3 天才看到，严重影响业务决策。

这时候 Spark 的优势就体现出来了。首先，使用 Spark 处理最底层日志的前几次统计聚合。如果聚合后数据量相对较小，再融合 Hive 数据任务进行处理。这样就能大幅提升数据产出效率，让数据能够及时为业务方所使用了。

-   **实时计算**
    

除了数据处理和统计，Spark 主要应用在互联网计算广告、准实时报表、机器学习等业务上。我们来看一下 Spark 在**广告业务**和**准实时报表**方面的运用。

**广告业务**：广告业务作为最依赖算法模型的业务，需要大量的互联网模型训练和预估，中间经过无数数据迭代，一份数据会被切分成很多部分，经历过很多轮训练。这时候 Spark 以最快速度完成模型训练的优势就能体现出来了。

**准实时报表**: 业务方希望实时掌控最近每分钟，App 各个功能模块被使用的次数，即沿着时间轴，按每分钟被切分成一段段，希望了解每分钟产生的各维度指标的统计。这个 Hive 完全无法完成。Spark Streaming 是专门为此场景设计的流式计算解决方案之一。

### Spark 专用编程语言 Scala

Spark 使用 Scala 语言进行实现。这里简单介绍下 Scala 语言。Scala 来源于 Java，但 Scala使用函数式编程思维来开发程序。 另外 Scala 程序可以编译成 .class 文件在 JVM 上运行。这点与 Java 相同，这两种语言大部分 API 是可以相互调用的。

Scala 学习门槛相比 Java 来说要高一些。这里主要介绍与 Scala 的数据处理相关的函数和方法，目的在于带你了解 Scala 基本用法，从而方便学习 Spark 数据处理部分。对于 Scala 高阶函数使用方法感兴趣的同学，可以参考本文最后推荐的网站进行学习。

#### 表达式

我们几乎可以说 Scala 一切都是表达式。

res1 是解释器自动创建的变量名称，用来指代表达式的计算结果。它是 Int 类型，值为3。

#### 变量类型

Scala 的变量类型只有两种类型：**val**和**var**。val 为不可变变量，我们来看下面的例子：

```
scala> val a = 100
a: Int = 100
scala> val a = "hello scala"
a: String = hello scala
scala> val a = 1 + 2
a: Int = 3
scala> a = 1
<console>:25: error: reassignment to val
       a = 1
         ^
```

var 为可变变量，即声明后可以再次修改的变量， 举例：

```
scala> var a = 100
a: Int = 100
scala> a = 99
a: Int = 99
scala> a = "hello"
<console>:25: error: type mismatch;
 found   : String("hello")
 required: Int
       a = "hello"
           ^
```

注意：在数据处理过程中，能使用 val 的时候，尽量不使用 var。这是为了让多线程中的并发访问保持读写一致性。可变变量的值可以被其他逻辑修改，常量则不能被修改。因此整体上更为稳定一些。

#### 数据类型

Scala 的基础数据类型，如表格所示，主要包括 Byte、Short、Int、Long、Float、Double、Char、String、Boolean、Unit、Nul、Nothing 等。

![Lark20201009-154106.png](https://s0.lgstatic.com/i/image/M00/5B/E0/CgqCHl-AFEqAeB85AAETKXlAa00248.png)

#### 基础数据结构

Scala 基础数据结构还包括：数组、列表、元组、集合，和其他高级语言类似。简单举个例子：

```
val arr = new Array[Int](3)
scala> val arr = new Array[Int](3)
arr: Array[Int] = Array(0, 0, 0)
scala> arr(0) = 10
scala> arr(1) = 20
scala> arr(2) = 30
scala> arr
res5: Array[Int] = Array(10, 20, 30)
scala> val list = List(10,20,30)
list: List[Int] = List(10, 20, 30)
scala> list.sum
res10: Int = 60
scala> val tuple = ('a', 100, 'b', 99)
tuple: (Char, Int, Char, Int) = (a,100,b,99)
scala> var set = Set("wang", "li", "zhang")
set: scala.collection.immutable.Set[String] = Set(wang, li, zhang)
scala> set+="li"
scala> set
res7: scala.collection.immutable.Set[String] = Set(wang, li, zhang)
scala> set+="sun"
scala> set
res9: scala.collection.immutable.Set[String] = Set(wang, li, zhang, sun)
```

以上介绍了 Scala 的基础变量、数据类型和数据结构。其实函数方法调用对于每种数据结构都集成了，这里就不细说了。你可以在交互式命令行进行手敲学习，多实践几次就熟练了。  
有了对 Scala 语言基础认知，我们下面再开始介绍 Spark 如何进行数据处理的。

### Spark 数据处理实例

Spark 的核心是建立在统一的抽象 RDD (Resilient Distributed Dataset) 之上，RDD 全称为**弹性分布式数据集。** RDD 是一个不可变、可分区、里面元素可以并行计算的集合。它使 Spark 的各个组件可以无缝进行集成（现在 Spark 3.0 之后开始倡导 DataSet ，但理解 RDD 能够帮助我们更好地理解 Spark）。

Spark 数据处理的基本流程：第一步，创建 RDD；第二步，对 RDD 进行数据处理，并最终生成想要的数据结果。

#### 创建 RDD

我们可以从本机或者集群获取数据创建 RDD 。

第一，从本机创建 RDD。参考下面代码：

```
scala> val lines = sc.textFile("file:///usr/local/.../word.txt")
```

第二，从集群创建 RDD。以下代码进行了说明。

```
scala> val lines = sc.textFile("hdfs:///usr/xxx/.../word.txt")
```

第三，通过数组、集合、列表等创建 RDD。可以参考一下代码。

```
scala> val list = List(10, 20, 30)
list: List[Int] = List(10, 20, 30)
scala> val rdd = sc.parallelize(list)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[18] at parallelize at <console>:26
```

#### RDD 转换 ( transformations ) 和行动 ( actions )

详细可参考：[spark 官网：rdd-programming-guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)

transformations 常用的 API 有以下形式：

-   **map(func)**：通过 func 函数，对 RDD 数据转换生成新 RDD，可以简单理解为 Python 中 lambda 函数。
    
-   **filter(func)**：对于原有 RDD 中，满足经过 func 处理后返回 True 的数据保留下来，生成新数据集合。
    
-   **flatMap(func)**：将原有数据打平，可以简单理解为行转列。
    
-   **sample(withReplacement, fraction, seed)**：对原有 RDD 抽样，生成新 RDD  
    union(otherDataset)： 将两个 RDD 合并后生成新 RDD。
    
-   **reduceByKey(func, \[numPartitions\])**：针对( K, V )样式 RDD，将 K 作为被聚合的 key，计算 V 的值。
    
-   **repartition(numPartitions)**：将原有 RDD 重新组合到不同的分区中，比如原来 RDD 在 1000 个分区上，小文件过多，但实际只有几 MB 数据，这时一般会 reshuffle 到一个分区即可。
    

actions 常用 API 主要有以下形式：

-   **reduce(func)**：使用函数来聚合 RDD 相关的元素。
    
-   **collect()**：将数据集的所有元素作为数组返回。
    
-   null  
    **count()**：返回 RDD 元素数量。
    
-   **first()**：取 RDD 第一个元素。
    
-   **take(n)**：取 RDD 第 N 个元素。
    
-   **saveAsTextFile(path)**：将处理后的 RDD 保存到 HDFS 路径。
    
-   **foreach(println)**：对 RDD 中每个元素，使用 func 进行处理，比如打印每个元素。
    

RDD 转换操作，即 transformations 操作，是指将 RDD 由一种数据结构样式转换成另外一种样式，每次转换会生成新的 RDD 供下次 transformations 使用。需要注意的是，**通过transformations 生成的 RDD 是惰性求值的**，可以简单理解为一个 RDD 经过各种transformations ，只是记录了数据转换的过程，不会触发计算。只有最终遇到 actions 操作，才会发送真正的计算。

接下来我们通过实际案例，了解下 transformations 和 actions 函数的使用。

#### 数据源介绍

```
scala> val rdd = sc.textFile("/Users/leihao1/data/qujinger/专栏课程/example-city")
scala> rdd.take(3).foreach(println)
2019/8/14浙江省861225
2019/10/24广东省767495
2020/3/18河北省767495
```

数据源中共三列，分别是：日期、省份、用户访问次数。下面通过 Spark 来完成下面几个统计任务：

1.  数据总行数。
    
2.  各省份的访问次数总和。
    
3.  过滤出大于 50 万次访问次数的省份。
    

这些都是简单而普遍的统计场景，我们看看 Spark 是如何完成的。

**第一步：读取数据。**  
Spark 在进行数据处理前，也需要先加载数据，可以从本机读取数据，也可以从分布时集群获取数据。

下面演示的是读取本地的练习数据。如果希望读 HDFS 数据，只需要将本地路径替换成 HDFS 路径即可。

```
scala> val rdd1 = data.map(line => line.split("\t")).map(arr => (arr(1), arr(2)))
rdd1: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[26] at map at <console>:27
scala> rdd1.take(3).foreach(println)
(浙江省,861225)
(广东省,767495)
(河北省,7674)
```

**第二步：行数统计**。  
数据分析师最常用聚合函数完成数据统计。比如，查询一个表的总函数，计算某个字段数据之和。Spark 也有类似的基础功能。下面，我们通过 Spark 来统计数据源的总行数，以及 pv 字段的加和，也就是看总的浏览次数。

```
scala> rdd1.count
res33: Long = 20
scala> rdd1.map{case(city, pv) => pv.toLong}.reduce(_+_)
res37: Long = 7806
```

**第三步：分组聚合。**  
Spark 分组聚合函数是用 reduceByKey 实现的，类似 SQL 语言里的 group by 用法，但又远比 SQL 功能强大。下面我们通过 reduceByKey 来分组汇总下各个省份的总流量。

```
scala> rdd1.map{case(province, pv) =>
     | (province, (1, pv.toLong))
     | }.reduceByKey{case((n1, pv1), (n2, pv2)) =>
     | (n1+n2, pv1+pv2)
     | }.foreach(println)
(湖北省,(1,541376))
(江西省,(1,371881))
(浙江省,(1,861225))
(湖南省,(1,79023))
(江苏省,(1,371881))
(华盛顿,(1,371881))
(重庆,(1,371881))
(山东省,(1,541376))
(河北省,(1,767495))
(广东省,(4,1759775))
(上海,(6,1396543))
(河南省,(1,3718
```

**第四步：完成过滤**。  
数据分析师在查询数据的时候，都会用到条件过滤来获取满足自己查询目标，即 SQL 语言中的 where 条件语句。在 Spark 中的过滤方式有很多种， 其中最常用的是 filter 函数。下面我们，通过 filter 函数来过滤出满足 50 万浏览量的数据行：

```
scala> rdd1.filter{case(_, pv) => pv.toLong>=500000}.foreach(println)
(浙江省,861225)
(广东省,767495)
(河北省,767495)
(山东省,541376)
(广东省,541376)
(上海,541376)
(湖北省,541376)
```

以上，通过 Spark 来完成数据分析师熟悉的几个统计 case ，相信你会对 Spark 数据处理有更直观的认识。在实际工作中，Spark 处理的数据量和复杂度可能会更高。在提交 Spark 任务时，肯定会涉及基础资源配置、内存调优、数据倾斜处理等问题，这里不展开讲了，我们只需要了解，Spark 基本上就是按照上面方式，再进行组合和优化，完成基础数据处理的。

### 总结

本课时先介绍到这里，希望通过学习，能让你了解到 Spark 的优势，使用场景，以及如何使用 Spark 进行大数据处理。非常感谢你的学习，如果有关于 Spark 数据处理的一些问题，欢迎在留言区提问。

最后给你推荐一些学习网站。Spark 生态很强大，对于数据分析师来讲，可以重点了解 Spark 数据处理的部分。对机器学习、流失计算感兴趣的同学，则可以进一步学习 MLlib、Spark Streaming。而官网是最好最全面的学习文档，感兴趣的同学可以点击以下链接，通过网站进行进一步学习。

-   [Spark 官网：Examples](http://spark.apache.org/examples.html#)。
    
-   [Spark 入门中文文档](https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/programming-guide/quick-start.html)。
    
-   [Scala 官网](https://docs.scala-lang.org/overviews/scala-book/introduction.html)。
    
-   [twitter scala 课堂](http://twitter.github.io/scala_school/zh_cn/)。

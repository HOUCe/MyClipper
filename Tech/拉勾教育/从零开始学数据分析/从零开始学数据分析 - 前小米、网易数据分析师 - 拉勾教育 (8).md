---
created: 2021-10-11
tags: []
source: https://kaiwu.lagou.com/course/courseInfo.htm?courseId=500#/detail/pc?id=4789
author: 
---

# [从零开始学数据分析 - 前小米、网易数据分析师 - 拉勾教育](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=500#/detail/pc?id=4789)


你好，我是取经儿。上一课时我带你学习了快速搭建 BI 平台来构建报表。本课时我将手把手教你在本地电脑安装离线大数据处理工具 Hive。Hive 也是数据分析师最常使用的工具。在本机部署Hive ，可以帮助没有数据工作经验的同学快速上手。而对于已经参加工作的数据分析师来讲，本地 Hive 在语法的测试验证方面也会更方便快速。

下面我们讲下如何在本地 Linux 或者 MacOS 安装 Hive。

### Hive 安装流程

Linux 或 MacOS 安装 Hive主要依赖以下环境：

-   Java；
    
-   Hadoop；
    
-   MySQL；
    
-   MySQL-Connector。
    

首先 Hive 是 Hadoop 生态系统数据仓库工具。Hive 运行依赖 Java 环境，属于 Hadoop 重要组件。 同时，Hive 元数据一般使用 MySQL 来存储，所以在安装 Hive 之前需要先安装 Java、Hadoop、MySQL 。

注意：本课时涉及安装下载的软件，为了方便，我会为你提供两种方式下载。第一种是官网下载链接；第二种是我出于速度考虑，特意帮你准备的百度网盘下载链接。

下面我们一步一步部署本机安装 Hive 所需要的环境。

#### 第一步：安装 JDK。

先检查本机是否安装 Java。使用下面命令，如果显示 Java 版本在 1.8 以上，即可继续第二步安装。

```
workindata:local qujinger$ java -version
java version "1.8.0_261"
Java(TM) SE Runtime Environment (build 1.8.0_261-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.261-b12, mixed mode)
```

如果本机未安装 Java，则按下面步骤先安装 Java。

1.下载 JDK。

-   [方式一：JDK 官网下载](https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html)。
    
-   [方式二：JDK 百度网盘](https://pan.baidu.com/s/1gIbsNywganTA5q26cpYcEA)（密码：okb7）。
    

2.解压JDK并将解压文件夹移动到 /usr/local。

```
tar -xvf jdk-8u161-linux-x64.tar.gz -C /usr/local
```

3.配置环境变量。

```
vim ~/.bashrc
export JAVA_HOME=/usr/local/Java/jdk1.8.0_181/
export JRE_HOME=$JAVA_HOME/jre
export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH
export CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:.
source ~/.bashrc
```

4.测试 Java 环境生效。

```
workindata:local qujinger$ java -version
java version "1.8.0_261"
Java(TM) SE Runtime Environment (build 1.8.0_261-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.261-b12, mixed mode)
```

以上过程后，本机便顺利安装上了 Java 环境，下面我们来安装 Hadoop。

#### 第二步：安装 Hadoop。

Hive 运行依赖于 Hadoop 工作环境，所以需要先在本机搭建 Hadoop 环境，以下是介绍如何在本机上搭建 Hadoop 环境。

首先，下载 Hadoop，并解压到/usr/local路径里。

-   [方式一：Hadoop官网下载](https://hadoop.apache.org/releases.html)（选择 Binary 文件进行下载）。
    
-   [方式二：百度网盘](https://pan.baidu.com/s/1gIbsNywganTA5q26cpYcEA)（密码：okb7）。
    

```
sudo tar -zxf ./hadoop-3.2.1.tar.gz -C /usr/local
```

1.设置环境变量。

```
vim ~/.bashrc
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:/usr/local/hadoop/bin
export PATH=$PATH:/usr/local/hadoop/sbin
source ~/.bashrc
```

2.检测 Hadoop 安装状态。

```
workindata:local qujinger$ hadoop version
Hadoop 3.2.1
Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842
Compiled by rohithsharmaks on 2019-09-10T15:56Z
Compiled with protoc 2.5.0
From source with checksum 776eaf9eee9c0ffc370bcbc1888737
This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.2.1.jar
```

以上为安装 Hadoop 步骤。在 Hadoop 配置完成后，下一步安装 MySQL，用于 Hive 元数据管理。

#### 第三步：安装 MySQL。

在 CentOS 系统中安装 MySQL，安装步骤可以参考 07 课时《07 一小时搭建可视化 BI 数据平台》，我在那一课时中介绍了为 CentOS 系统安装 MySQL 的步骤。

在 MacOS 系统中安装 MySQL，则要先下载安装包。

-   [方式一：官网下载](https://dev.mysql.com/downloads/mysql/)（选择 DMG 格式下载）。
    
-   [方式二：百度网盘](https://pan.baidu.com/s/1gIbsNywganTA5q26cpYcEA)（密码：okb7）。
    

下载完后需要进行安装。这时候，按照提示一步步安装即可：

![image (1).png](https://s0.lgstatic.com/i/image/M00/62/1E/Ciqc1F-RVPSATinnAAKgksQ7PBY808.png)

注意：MacOS 安装 MySQL 完成后要启动 MySQL。以下是 MacOS 系统启动、停止、重启 MySQL 服务的命令。

```
sudo /usr/local/MySQL/support-files/mysql.server start
sudo /usr/local/mysql/support-files/mysql.server stop
sudo /usr/local/mysql/support-files/mysql.server restart
```

安装完成后，我们需要在 MySQL 创建新用户。新用户的用户名和密码都设置为 Hive。操作命令如下：

```
mysql -uroot -p 
mysql> create user 'hive'@'%' identified by 'hive';
mysql> grant all on *.* to hive@localhost identified by 'hive';   
# 后面的 hive 是配置 hive-site.xml 中配置的连接密码
mysql> flush privileges;  
# 刷新权限
```

#### 第四步：下载 Hive 安装软件。

-   [方式一：官网下载](http://www.apache.org/dyn/closer.cgi/hive/)。
    
-   [方式二：百度网盘](https://pan.baidu.com/s/1bTNYlPJqSmTlnsx3fKP09w)（密码：fvfk）。
    

解压 Hive 安装包并移动到 /usr/local 中即可。

```
sudo tar -zxvf ./apache-hive-3.1.2-bin.tar.gz 
sudo mv apache-hive-3.1.2-bin /usr/local/  -- copy到/usr/local目录
sudo mv apache-hive-3.1.2-bin hive       
```

#### 第五步：配置环境变量。

```
vim ~/.bashrc
export HIVE_HOME=/usr/local/hive
export PATH=$PATH:$HIVE_HOME/bin
export HADOOP_HOME=/usr/local/hadoop
source ~/.bashrc
```

#### 第六步：下载 mysql-connector-java.jar。

-   [方式一：官网下载](https://dev.mysql.com/downloads/connector/j/)。
    
-   [方式二：百度网盘](https://pan.baidu.com/s/1hkAENQCqliGvKa1zckFvbg)（密码：8s0s）。
    

下面将下载的jar文件Copy到路径/usr/local/hive/lib 中。

```
cp mysql-connector-java-8.0.21.jar /usr/local/hive/lib
```

#### 第七步：启动 Hive。

```
$ start-dfs.sh
$ $ hive
SLF4J: Class path contains multiple SLF4J bindings.
...
...
Logging initialized using configuration ...
Loading class `com.mysql.jdbc.Driver'. ...
Hive Session ID = fbeb3b3b-23d6-41e0-bbd2-8563f623ed29
Hive-on-MR is deprecated in Hive 2 and ...
hive>
```

启动 Hive 的过程中，可能遇到一些问题。经哥将这些问题和解决方法进行了汇总。感兴趣的同学可以去下面的链接看一看：  
[解决 Hive 启动报错：java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument](http://www.shangdixinxi.com/detail-1151470.html)。

这通常是因为 Hive 内依赖的 guava.jar 和 Hadoop 内依赖的 guava.jar 版本不一致造成的。

```
ll /usr/local/hadoop/share/hadoop/common/lib | grep guava
-rw-r--r--@ 1 1001  1001  2747878  9 10  2019 guava-27.0-jre.jar
ll /usr/local/hive/lib | grep guava
-rw-r--r--@ 1 qujinger  staff   2308517  9 27  2018 guava-19.0.jar
cp /usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/hive/lib/
rm /usr/local/hive/lib/guava-19.0.jar
```

-   解决 Hive 启动报错：metadata.SessionHiveMetaStoreClient，参考 stackoverflow 网站一个[解决方法](https://stackoverflow.com/questions/35449274/java-lang-runtimeexception-unable-to-instantiate-org-apache-hadoop-hive-ql-meta)。
    

```
$ cd /usr/local/hive/bin
$ ./schematool -dbType mysql -initSchema  
$ ./schematool -dbType mysql -info
```

-   Hive 创建数据库报错：Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask。
    

```
hive> create database tmp_db2;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Unable to create database path file:/user/hive/warehouse/tmp_db2.db, failed to create database tmp_db2)
hadoop fs -mkdir   -p  /user/hive/warehouse
sudo chown -R qujinger:wheel /user
hive> create database tmp_db2;
OK
Time taken: 0.333 seconds
hive> show databases;
OK
default
tmp_db2
Time taken: 0.513 seconds, Fetched: 2 row(s)
```

#### 第八步：创建本地 Hive 外表。

由于是本机安装，因此我们没有真正分布式的 HDFS 文件，这时候需要根据本地文件来制定Hive 外表。

```
cat /tmp/test_data.txt |head -3
2019/10/1151
2019/10/2249
2019/10/3350
```

这里需要使用 Hadoop 命令 HDFS 来创建目录以及将测试数据 Copy 到指定路径，用于生成 Hive 外表。注意：必须使用 Hadoop 命令进行，如果使用本地 Shell 命令 Copy 数据，无法完成表创建和使用。

```
$ hdfs dfs -mkdir -p /usr/local/hive/tmp_db/order_info   
$ hdfs dfs -cp /tmp/test_data2 /usr/local/hive/tmp_db/order_info
hive> CREATE EXTERNAL TABLE order_info (dt string, id int, order_num int)
    > COMMENT 'user orders'
    > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
    > STORED AS TEXTFILE
    > LOCATION '/usr/local/hive/tmp_db/order_info';
OK
Time taken: 0.086 seconds
```

#### 第九步：本地 Hive 进行基础统计。

下面我们就可以利用本机安装的 Hive 进行数据统计了。你可以尝试统计上面新建表的数据，比如：统计表格中的每月订单量。

```
hive> set hive.cli.print.header=true;
hive> select split(dt, '/')[1] as dt, sum(order_num) order_num, min(order_num) as min_order_num, max(order_num) as max_order_num from order_info group by split(dt, '/')[1] order by dt;
Query ID = qujinger_20201018120317_c81beb8f-8c55-4869-aa75-9d510026a1b3
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
...
...
2020-10-18 12:03:20,995 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_local183869176_0006
...
Total MapReduce CPU Time Spent: 0 msec
OK
dtorder_nummin_order_nummax_order_num
1020914590
1121895798
Time taken: 3.226 seconds, Fetched: 2 row(s)
```

经过以上步骤，想必你已经在本机成功安装了 Hive。现在，你可以在本地创建数据库和表格，并对其进行基础统计，从而进行练习，加深学习效果。在这个过程中，你也可以及对 Hive 基础语法、函数进行学习和巩固练习了。

### 总结

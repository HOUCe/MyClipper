---
created: 2021-10-11
tags: []
source: https://kaiwu.lagou.com/course/courseInfo.htm?courseId=575#/detail/pc?id=5870
author: 
---

# [万能的数据分析法则 - 同花顺高级数据分析师 - 拉勾教育](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=575#/detail/pc?id=5870)


前几讲介绍了几种具体的分析类型，这一讲我们来聊一聊 A/B 测试。

和前面几节不同，A/B 测试并不是一个完整的业务数据分析流程，而是穿插在其中的一种分析方法。但是它在数据分析的工作当中实在是太常见了，并且看起来很简单，但其实有一些坑，所以我准备单独用一讲的时间，给你介绍一下 A/B 测试相关的知识。

### 怎么做 A/B 测试

A/B 测试在目前的互联网行业内已经非常普遍了。基本的做法，是为网页或者 App 的页面制作两个或多个不同的版本，然后在同一时间，让相同属性的用户分别访问两个不同的版本。在这个过程中，收集用户的行为数据和业务数据，最终通过不同版本之间的数据对比，经过统计学的检验，评估出哪一个版本更好。

这里有几个操作的要点。

#### 1\. 同一时间

第一个操作要点是不同版本的测试要在同一时间。现在的热点变化非常快，如果你不是同一时间，很难说清楚数据的变化到底是因为市场热点的变化导致的，还是因为测试的版本真的有效。这一点也是互联网公司相对传统公司的优势所在。

传统企业比如线下经销商，做促销活动的时候是没办法区分不同用户的。只要在门店做了宣传，那么所有经过这家店的用户都会看到宣传的内容。如果想要对比两个不同的活动的效果，就只能等这次活动结束之后再开展新的活动。但是营业额不单单是由促销活动带来的，天气情况、学生放假、流行趋势等等都会影响营业额，甚至这次活动也会影响下一次的活动。

所以不同时间总结出来的经验未必准确，所以传统企业的知识积累非常慢，一些有效的运营方式要经过好几年的时间才能总结出来。

#### 2\. 同类用户

第二个操作要点是不同版本之间的对比，必须确保用户的属性是相同的。

比如用户属性有新用户和老用户两种，我们在进行两个版本的 A/B 测试的时候，就要确保不同版本之间的用户构成是类似的。如果 A 版本的用户中新用户的占比更高，而新用户的转化率又较低，那么 A 版本的整体转化率就肯定要比 B 版本更低。甚至还会出现以下这种情况：

![Drawing 0.png](https://s0.lgstatic.com/i/image2/M01/0B/06/CgpVE2ATt2GAAuOAAABY6KFEv_A912.png)

比如表中的数据，A 版本的整体转化率 18%，B 版本的整体转化率 18.5%。A 版本的整体转化率要比 B 版本低。但是实际上 A 版本的新用户转化率 10%，老用户的转化率 21.4%，单独看这两个转化率都要比 B 版本更高。

如果用户的属性相同的话，就不会出现这样的问题了。

要确保用户属性相同，就要确保分发算法的随机性。比较简单的可以依据 IP 地址、用户 id 等，分成 2 组就按照奇偶数，分成 n 组就除以 n 取模，比如分成 3 组就除以 3，余数 0、1、2分别作一组。比较复杂的方案一般是通过哈希算法进行随机分配，这样的好处是可以做分层正交，多组试验不用相互干扰。

不过这方面知识主要是开发人员需要掌握的，数据分析师了解即可。目前业内比较成熟的 A/B 测试平台，如热云、云眼等都是用这种算法。

#### 3\. 统计学检验

第三个操作要点是 A/B 测试最终的结果需要经过统计学的检验，这一点是非常容易被大家忽视的。有些数据看起来差距很明显，但由于样本量较小，或者差距不够明显，还不能认定是有效的。

比如一个转化页，每天有 1 万人的流量。我们选出其中的 10%，也就是 1000 人，作为实验组，投放新版本的页面。剩下的 9000 人作为对照组，继续使用原来的版本的页面。投放后的数据回收结果如下：

![Drawing 1.png](https://s0.lgstatic.com/i/image2/M01/0B/06/CgpVE2ATt22Adp-7AABQhsVCxIY165.png)

对照组投放 9000 人，转化 310 人，转化率 3.44%。

测试组投放 1000 人，转化 43 人，转化率 4.3%

测试组相比对照组转化率提升了 24.84%。

一般人拿到这个数据，就直接下结论了：实验组相比对照组转化率提升 24.84%。但是真的是这样吗？

我们必须通过统计学的方式，计算出显著性水平。一般只有显著性水平大于 0.95，才能认为结论是正确的。你可以理解成，有 95% 以上的把握认为这个结论是成立的。

如果不想学习计算的具体方法，甚至连原理也懒得了解，那么最简单的办法就是搜索“A/B测试结果分析工具”，有很多网站会提供一个计算器，你直接输入访问数、转化数，就会自动计算两个版本的转化率之间的显著性水平。

我们把之前的数字输入到这个计算器当中，会出现这样的结果。

![Drawing 3.png](https://s0.lgstatic.com/i/image2/M01/0B/06/CgpVE2ATt3eAMa2yAAKwlOjRBvg188.png)

结果显示，我们 89.93% 肯定实验组的改动会带来转化率的提升。这个数据没有到 95%，因此我们不能认为实验组就一定要比对照组更好。

#### 4\. 灰度发布

第四个操作要点是 A/B 测试要考虑是否需要灰度发布。灰度发布是指能够平滑过渡的一种发布方式。如果 A/B 测试的时候做灰度发布，可以让一部分用户进入测试的 A 版本，比如 5%。另一部分用户继续使用原来的版本，也就是剩下的 95%。

如果 A 版本确实效果更好，那么逐步扩大范围，从 5% 扩大到 10%，再扩大到 20%、30%、40%，直到把所有用户都迁移到 A 版本。

这样做的好处是如果新版本有重大的问题，小范围的测试不会造成全局的巨大影响。大公司一般所有的业务改动都需要做 A/B 测试，而且一般都会进行灰度发布。因为大公司的业务体量比较大，一个小小的改动也会带来业务量的巨大变化。如果一天有 1 个亿的营业额，一个小改动影响了 1%，那也会影响 100 万的销售额。

不经过测试就直接上线的话影响就太大了，比较好的做法是先给一小部分流量做测试，比如先在全体用户中划出一部分作为测试样本，这样就算新版本效果不好，做测试的一小部分用户营业额下降 1%，对全局来说影响也不大。

#### 5.指标的选择

第五个操作要点是指标的选择。A/B 测试一般比较容易找，结合测试页面的目的就可以，比如落地页上的 A/B 测试一般都以落地页的转化率作为衡量指标，首页的 A/B 测试一般以用户留存率作为衡量指标等，指标的选择可以参考[第 3 讲](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=575#/detail/pc?id=5873)中的 OSM 模型。

这边要提一下的是，A/B 测试不仅要关注当前页面的指标，还要考虑全局，否则会陷入局部更优，而全局变差的情况。比如一个购买转化过程，要经过落地页、支付页、支付成功三个步骤。我们在落地页上进行 A/B 测试，一般参考的是落地页到支付页的转化率，哪个版本的转化率更高，那么就用哪个版本。

但是我们还得参考一下全局的指标。落地页不仅仅要考虑的是更多人跳转支付页，最终的目标应该是为了让更多用户付费成功，所以如果跳转支付页的比例提高了，而整体付费成功的比例下降了，那这个版本依然是一个失败的版本。

所以在 A/B 测试的指标选择上，不能太盯着局部，也要参考整体效果。

### A/B 测试的类型

A/B 测试的类型一般有：

-   功能 MVP；
    
-   路径测试；
    
-   文案测试；
    
-   设计测试。
    

#### 功能 MVP

功能 MVP 就是一个新功能上线前，先通过 MVP（最小可行化产品）的形式进行测试，看看用户是否有这样的需求。

比如某 App 在首页，在顶部增加一个推荐栏。但是不确定用户是否有这样的需求，于是开展了一次 A/B 测试。

![Drawing 5.png](https://s0.lgstatic.com/i/image/M00/93/0F/Ciqc1GATt4qAEMOOAAYGLQl40zw177.png)

图中左边是原始版本，右边是测试版本。测试版本相比原始版头多了一个“顶部推荐栏”。

最终，试验版本的各项指标明显优于原始版本，人均文章的浏览量大幅提升。

#### 路径测试

路径测试就是比较不同的步骤和路径对用户的影响。这里的步骤和路径不仅包括跳转的顺序，也包括单个页面上各模块顺序的调整。

比如某外语培训公司在外部投放的一个落地页上开展了一次 A/B 测试。

![Drawing 7.png](https://s0.lgstatic.com/i/image/M00/93/1A/CgqCHmATt5OAdL7uAAiy1LcE9S4482.png)

图中左边是原始版本，推广页第三屏展示内容为“口译课程介绍”；右边是试验版本，将原本的“口译课程介绍”放到后面，把“各类课程介绍”提前至第三屏，其余页面元素保持不变。

改版后，试验版本的页面转化率效果更好。

#### 文案测试

文案测试是比较不同的文案对用户的使用是否会产生影响。文案测试很多时候测的是用户心理学，改变用户对产品的认知。

比如某 K12 在线教育机构的一次落地页文案试验：

版本一中 Banner 的文案为“一次注册，让孩子一生与众不同”；版本二中的 Banner 文案为“直播+辅导，学习更有效”。其余元素没有任何改动。

![Drawing 9.png](https://s0.lgstatic.com/i/image2/M01/0B/04/Cip5yGATt5uANpURABCdWDV6hXw111.png)

这次试验的结果是试验版本（直播+辅导，学习更有效）相比于原始版本（一次注册，让孩子一生与众不同）注册转化率明显提升。

#### 设计测试

设计测试是比较不同的视觉方案对用户的影响。

比如墨迹天气对分享按钮的形状进行了一次测试，一共四组方案。

方案一：原始版本，分享图标样式为两条直线连接的三个空心圆。

方案二：相比原始版本，分享图标为圆形由空心变为实心。

方案三：分享图标为长方形上部空出+向上的箭头。

方案四：分享图标为长方形右上角空出+向右上方的箭头。

![Drawing 11.png](https://s0.lgstatic.com/i/image2/M01/0B/04/Cip5yGATt6WARcmwAAyk58_Yraw558.png)

最终相比其他版本，方案 3 中分享图标的点击率上涨了近 20%，这对用户量过亿的墨迹天气来说，也算是一个不小的提升；

### 容易忽视的错误

介绍完 A/B 测试的常见类型，接下来说一些容易被我们忽视的错误。

以上介绍的四种类型的 A/B 测试，其实是有优先级顺序的，优先级从上到下依次递减。这样的顺序是符合产品设计的规律的：

> 如果一个产品的功能本身不被用户接受，你文案吹得再好，设计得再好看也没用，产品满足需求永远是第一位的。其次是产品功能之间的结构关系，如何跳转，页面顺序等。再之后才是文案和设计。

很多人学了 A/B 测试的方法之后，盲目迷信 A/B 测试，把所有的优化分析工作都交给 A/B 测试来完成。有灵光一闪的想法就用 A/B 测试来试一下，而各个想法之间没有关联，无法串联起一条主线。看到一个案例提到按钮颜色的测试案例，就想着自己也试一下不同的按钮颜色。看到别人页面顺序调整的成功案例，于是自己也调一下页面顺序。至于自己是不是应该做按钮颜色的测试，根本没有思考。

虽然按钮的颜色试验测试之后可能确实会对转化率有一定的帮助，但是这样的 A/B 测试效率一定不是最高的。那些成功案例里其他公司做按钮颜色的测试，可能是这家公司产品功能、页面结构等已经优化得很好了，实在找不到优化点于是开始对设计层面进行测试，并且有可能已经搭建了完善的 A/B 测试体系，每一个实验的成本很低，所以才有精力对这些细节做测试。

你不要只看到他吃第 6 个饼的时候吃饱了，就想着直接去吃第 6 个饼，要知道他前面还吃了其他的 5 个饼。

字节跳动是典型的依靠大量的试验成长起来的公司，就连“今日头条”这个名字也是依靠 A/B 测试决定的。但是我们不能只学其形，不学其神。虽然字节跳动以试验驱动业务，但是以理性著称的字节跳动的 CEO 张一鸣也表示“同理心才是产品的根基，A/B 测试只是工具”。所以千万不要迷信 A/B 测试，它只是工具。

### 小结

小结一下今天的课程。A/B 测试是互联网行业非常常见的一种优化方法，在分析 A/B 测试的结果时，我们必须确保实验是同一时间、同类用户，并经过了统计学检验后才能下结论。如果影响的用户量较大，还需要进行灰度发布，逐步提高测试版本的覆盖人数。

A/B 测试常见的类型有：功能 MVP、路径测试、文案测试、设计测试，这几类测试的优先级是逐步递减的。在实际工作中，不要迷信 A/B 测试，而是要有对产品和业务的理解，以业务理解作为主线，A/B 测试只是工具。

恭喜你坚持到了这一讲，至此我们这个专栏的正课部分就全部结束了。下一讲是一节彩蛋课，我将讲解如何搭建数据运营体系，到时见。

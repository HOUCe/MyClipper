---
created: 2021-10-12
tags: []
source: https://kaiwu.lagou.com/course/courseInfo.htm?courseId=547#/detail/pc?id=5273
author: 
---

# [打造千万级流量秒杀系统 - 前小米国际电商技术负责人 - 拉勾教育](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=547#/detail/pc?id=5273)


前面我为你介绍了秒杀系统的开发和本地测试，测试通过后，我们还需要将服务部署到线上环境，并在活动开始前一天进行 SLB 预热和压力测试。为什么呢？这是因为大型促销活动流量非常大，需要确保线上环境的各资源能扛住压力，从而保障活动期间不出故障。但由于秒杀系统能承载的 QPS 太高了，在压力测试中普通的压测工具根本无法满足要求，于是就需要用到分布式压测工具了。

所以，接下来我就为你详细介绍下如何做 SLB 预热和分布式压测。

### 分布式压测

上一讲我实现了一个 bench 命令用于压测秒杀 API 服务，不过它是单节点压测工具，性能比较有限，无法压测秒杀集群，我们需要将它改造成分布式压测系统。

什么是分布式压测系统呢？简单来说，分布式压测系统是利用多个服务器节点，在集群管理单元的协调下，同时向被测服务发起压力测试，以便给被测服务制造超高压力。

#### 分布式压测系统原理

分布式压测系统需要满足什么样的基本条件呢？

第一，并发请求能力，这是压测工具最基本的功能。

第二，数据统计能力。压测工具在发起并发请求后，需要收集各请求的性能指标，比如请求延迟、QPS、错误率等。

第三，水平扩展能力。分布式压测系统要能压测不同并发能力的被测系统，这要求它必须能够通过水平扩展来控制自身并发能力。

第四，集群管理能力。分布式压测系统本身是有多个节点的，它需要有一个控制中心来统一控制各节点的任务下发和数据统计，以便让各节点同时开始任务，并在任务结束后上报统计结果。

#### 分布式压测系统设计与实现

基于前面我提到的四点要求，一个分布式压测系统该如何设计呢？分布式压测系统重点体现在“分布式”上，它主要包括**任务管理单元和任务执行单元**。其中，任务管理单元负责配置任务，并下发给各个任务执行单元。这个任务下发其实就是配置下发，也就是说我们也可以用 ETCD 来实现配置同步。

具体同步哪些配置呢？前面我实现的 bench 命令主要用了并发数、请求数、url、keepalive 这 4 个参数，但一个通用的压测系统还有更多的参数。比如当压测系统需要测一个 POST 接口时，它需要指定数据以及数据类型，比如指定数据类型为 "application/json"。另外，每次下发任务需要有一个唯一的版本号，以便各执行单元能按照版本号汇总测试结果。

由于参数较多，而且需要通过 ETCD 同步配置，因此我们需要定义一个结构体来保存这些参数。比如我定义了一个 Task 结构体，它包含这几个字段：

1.  ID 字段，用于表示唯一任务，我们可以简单地用时间戳当任务 ID，毕竟同一时间不会进行两场压力测试。
    
2.  Servers 字段，表示被测服务的地址。它有可能是个域名（如 "event.test.com"），也有可能是一批 IP 加端口的列表（如："192.168.1.2:8080,192.168.1.3:8080"），使用它压测服务既能通过 DNS 压测，又能绕过 DNS 压测。
    
3.  Path 字段，表示被测接口的路径，也就是哪个接口，如 /event/list。
    
4.  Method 字段，表示被测接口方法名 ，它的值可能是 POST、GET 还是 PUT 方法等。
    
5.  Data 字段，表示压测接口需要的数据，如 json 格式的数据 {"event\_id: 123,"goods\_id":456}。
    
6.  ContentType 字段，用于表示压测接口需要的数据格式，如 "application/json"。
    
7.  Concurrency 字段，用于表示并发数，如 100。
    
8.  Number 字段，用于表示请求数的，如 1000000（一百万）。
    
9.  Duration 字段，用于表示压测时长的，如 300（秒）。
    
10.  Status 字段，用于表示任务状态，0 为未开始，1 为执行中，2 为停止中，3 为已停止。
    

具体代码如下所示：

```
type Task struct {
   ID          int      `json:"id"`
   Servers     []string `json:"servers"`
   Path        string   `json:"path"`
   Method      string   `json:"method"`
   Data        string   `json:"data"`
   ContentType string   `json:"content_type"`
   Concurrency int      `json:"concurrency"`
   Number      int      `json:"number"`
   Duration    int      `json:"duration"`
   Status      int      `json:"status"`
}
```

为了便于任务管理单元将结构体编码成 json 字符串存放到 ETCD，然后执行单元从 ETCD 中取出配置并从 json 字符串解码到结构体，我给结构体中每个字段都加上了 json tag。

接下来，我们就可以实现从 ETCD 读写配置的代码。

读配置的代码实现比较简单，之前我们已经实现了秒杀集群的配置同步逻辑，我们直接复制过来修改下即可。大致实现逻辑如下：

1.  主要是将 ETCD key 改成压测系统用的 key /bench/task/config；
    
2.  将 tmpConfig 类型改成 Task；
    
3.  给函数加一个参数 callback 用于通知程序在配置更新后执行相应操作，比如结束当前正在进行的测试并开始新的测试；
    
4.  移除启动时从 ETCD 初始化配置的代码，避免启动时执行任务。
    
5.  最终我们得到一个用于从 ETCD 中同步压测系统配置的函数 watchTask。
    

代码示例如下所示：

```
func watchTaskConfig(callback func(cfg Task)) error {
var err error
   cli := etcd.GetClient()
   key := "/bench/task/config"
   update := func(kv *mvccpb.KeyValue) (bool, error) {
if string(kv.Key) == key {
var tmpConfig Task
         err = json.Unmarshal(kv.Value, &tmpConfig)
if err != nil {
            logrus.Error("update bench config failed, error:", err)
return false, err
         }
         logrus.Info("update bench config ", tmpConfig)
         callback(tmpConfig)
return true, nil
      }
return false, nil
   }
   watchCh := cli.Watch(context.Background(), key)
for resp := range watchCh {
for _, evt := range resp.Events {
if evt.Type == etcdv3.EventTypePut {
if ok, err := update(evt.Kv); ok {
break
            } else if err != nil {
break
            }
         }
      }
   }
return nil
}
```

接下来我们需要改造 doBench 函数，支持结束当前任务，开始执行新任务。具体思路是让其成为 Task 的方法，在函数里面判断 Status 的值。如果 Status 值不为 1，则退出当前任务并更新 Status 值为 3。代码示例如下：

```
func (t *Task) doBench() {
   wg := &sync.WaitGroup{}
   wg.Add(t.Concurrency)
for i := 0; i < t.Concurrency; i++ {
go func() {
for atomic.LoadInt32(&t.Status) == 1 {
         }
         atomic.StoreInt32(&t.Status, 3)
         wg.Done()
      }()
   }
   wg.Wait()
}
```

然后，我们需要有一个任务管理器 TaskManager，它有一个字段 task 用于表示当前正在执行的任务，以及一个互斥锁用于更新当前任务。它还需要有一个 onConfigChange 方法用于处理配置变更，如果当前任务 Status 为 1，需要将其修改为 2，并等待状态变为 3 后开始执行新的任务。具体代码如下：

```
type TaskManager struct {
   sync.Mutex
   task Task
}
func (tm *TaskManager) onConfigChange(task Task) {
if atomic.LoadInt32(&tm.task.Status) == 1 {
      atomic.StoreInt32(&tm.task.Status, 2)
   }
for atomic.LoadInt32(&tm.task.Status) == 2 {
      time.Sleep(time.Second)
   }
   tm.Lock()
   tm.task = task
   tm.Unlock()
if task.Status == 1 {
      tm.task.doBench()
   }
}
```

最后，我们就可以在 Run 方法中添加一行代码 watchTaskConfig((&TaskManager{}).onConfigChange) 来监控配置变更并更新任务状态。

至于将配置写入到 ETCD 的方法，有很多种。可以用 etcdctl 命令，可以用代码来控制。当然，比较好的方式还是用代码实现一个管理后台，以便后续可以扩展更多方便的功能。

### SLB 预热

SLB 作为 DNS 后第一层负载均衡器，它的性能在整个系统中非常关键，通常是由云厂商提供。比如 AWS 的弹性负载均衡器 ELB 就是 SLB，分为 NLB 和 ALB，它会根据流量大小自动扩容、缩容。

为什么需要在活动前对 SLB 做预热呢？那是因为平常流量小，云厂商会为 SLB 分配比较小的资源，当大流量来的时候，才会逐渐扩容。但是，扩容是需要时间的，像秒杀这么大的流量，扩容时间可能持续好几分钟。而秒杀的大流量通常是在秒杀活动开始前后 1 分钟内，此时扩容速度是远赶不上流量增长速度的。如果流量超过 SLB 承载能力，就会触发 SLB 的限流，影响活动效果。因此，通常需要提前制造压力触发 SLB 自动扩容，这便是 SLB 预热。

SLB 预热需要注意哪些事情呢？

首先需要跟云厂商确认两个参数：

1.  流量降下来多久后 SLB 会自动缩容，以便确认预热时间安排在活动开始前多少时间，以防预热过早导致后续流量下降后 SLB 缩容；
    
2.  单个 SLB 集群最多能承载多少流量，以便确认活动期间是否要采用多个 SLB，以防单个 SLB 容量不足。
    

其次，需要分别压测以下几个场景：

1.  带 DNS 通过 SLB 压测秒杀 API 服务；
    
2.  不带 DNS 通过 SLB 压测秒杀 API 服务；
    
3.  直接压测秒杀 API 服务。
    

之所以要测这几个场景，主要是为了对比看性能瓶颈在什么地方。理想情况下，直接压测秒杀 API 服务应该是性能最好的，其次是不带 DNS 压测，但这三种场景的结果不应该相差很大。如果第一种相比第二种 QPS 小很多，那么瓶颈在 DNS，需要想办法优化它。同理，如果第 2 种比第 3 种小很多，那么瓶颈在 SLB，同样需要想办法优化。如果是 DNS 的瓶颈，则需要考虑使用 CDN 做就近解析。如果是 SLB 的瓶颈，则需要考虑使用多个 SLB，使用 DDNS 轮询做负载均衡。

![Drawing 0.png](https://s0.lgstatic.com/i/image6/M00/09/7A/CioPOWA2DfyAMgBSAAQhwrKEam8519.png)

### 小结

这一讲我给你介绍了分布式压测系统的原理、压测过程的注意事项以及 SLB 预热，这是本专栏秒杀系统的最后一讲，也是秒杀系统项目最后一步。希望你在学完这一讲后，能自己手动实现一个简单的分布式压测系统。在实现分布式压测系统的过程，你将会加深对“三高”架构的理解。

接下来给你出个思考题：如果要绕过 SLB 直接压测秒杀系统多个节点，压测工具该如何实现呢？期待你在留言区讨论哦！

好了，这一讲就到这里了。下一讲作为结束语，我将结合自己的经历，和你聊聊程序员成长之路。希望对你的职业发展有所帮助。

源码地址：  
[https://github.com/lagoueduCol/MiaoSha-Yiletian/blob/main/cmd/bench.go](https://github.com/lagoueduCol/MiaoSha-Yiletian/blob/main/cmd/bench.go)
